{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwUVhLwotN+jXHhWH4ccEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meghana366/NLP/blob/main/exp_6_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv-AZZZLVl95",
        "outputId": "2883129f-1723-4d42-a6d0-0c6b64449062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.0)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy nltk\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Input text\n",
        "text = \"Alliance Universty is looking for many registrations for ALF .\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "\n",
        "# 1. Tokenization\n",
        "print(\"\\n1. Tokenization:\")\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "\n",
        "# 2. Stop Word Removal\n",
        "print(\"\\n2. Stop Word Removal:\")\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "print(filtered_tokens)\n",
        "\n",
        "# 3. Lemmatization\n",
        "print(\"\\n3. Lemmatization:\")\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n",
        "\n",
        "# 4. Stemming (using NLTK with spaCy tokens)\n",
        "print(\"\\n4. Stemming:\")\n",
        "stems = [stemmer.stem(token.text) for token in doc if not token.is_punct]\n",
        "print(stems)\n",
        "\n",
        "# 5. POS Tagging\n",
        "print(\"\\n5. POS Tagging:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"->\", token.pos_)\n",
        "\n",
        "# 6. Named Entity Recognition (NER)\n",
        "print(\"\\n6. Named Entity Recognition:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"->\", ent.label_)\n",
        "\n",
        "# 7. Dependency Parsing (Extra NLP task)\n",
        "print(\"\\n7. Dependency Parsing:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"->\", token.dep_, \"->\", token.head.text)\n",
        "\n",
        "# 8. Noun Chunking (Extra NLP task)\n",
        "print(\"\\n8. Noun Chunking:\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYCRywebZj0",
        "outputId": "593e7114-7efc-4d61-9a30-cec40f8610a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Alliance Universty is looking for many registrations for ALF .\n",
            "\n",
            "1. Tokenization:\n",
            "['Alliance', 'Universty', 'is', 'looking', 'for', 'many', 'registrations', 'for', 'ALF', '.']\n",
            "\n",
            "2. Stop Word Removal:\n",
            "['Alliance', 'Universty', 'looking', 'registrations', 'ALF']\n",
            "\n",
            "3. Lemmatization:\n",
            "['Alliance', 'Universty', 'be', 'look', 'for', 'many', 'registration', 'for', 'ALF', '.']\n",
            "\n",
            "4. Stemming:\n",
            "['allianc', 'universti', 'is', 'look', 'for', 'mani', 'registr', 'for', 'alf']\n",
            "\n",
            "5. POS Tagging:\n",
            "Alliance -> PROPN\n",
            "Universty -> PROPN\n",
            "is -> AUX\n",
            "looking -> VERB\n",
            "for -> ADP\n",
            "many -> ADJ\n",
            "registrations -> NOUN\n",
            "for -> ADP\n",
            "ALF -> PROPN\n",
            ". -> PUNCT\n",
            "\n",
            "6. Named Entity Recognition:\n",
            "Alliance Universty -> PERSON\n",
            "ALF -> ORG\n",
            "\n",
            "7. Dependency Parsing:\n",
            "Alliance -> compound -> Universty\n",
            "Universty -> nsubj -> looking\n",
            "is -> aux -> looking\n",
            "looking -> ROOT -> looking\n",
            "for -> prep -> looking\n",
            "many -> amod -> registrations\n",
            "registrations -> pobj -> for\n",
            "for -> prep -> registrations\n",
            "ALF -> pobj -> for\n",
            ". -> punct -> looking\n",
            "\n",
            "8. Noun Chunking:\n",
            "Alliance Universty\n",
            "many registrations\n",
            "ALF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello!!! ðŸ˜ŠðŸ˜Š This   is   an NLP   project!!! ### 2024\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove emojis and special characters\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "# Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWsbqxYPbZgR",
        "outputId": "83553d9f-cfb6-4ee6-91a4-59b7fe109db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello this is an nlp project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing the redundancy\n"
      ],
      "metadata": {
        "id": "lkFlNzSGsjEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"helooooo this is soooo cooool\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Reduce repeated characters to one\n",
        "text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHCBchxkbZd-",
        "outputId": "23b6c5e7-d98a-4d6d-dbda-87311ae9da73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helo this is so col\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OuezX4asbZbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RbvPD6ivbZY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}